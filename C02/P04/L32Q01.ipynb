{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Counting Model Performance Thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:52:45.899940Z",
     "start_time": "2025-06-03T20:52:45.896270Z"
    }
   },
   "source": [
    "model_performance = {\n",
    "    'Experiment 1': {\n",
    "        'Model A': 0.85, 'Model B': 0.9, 'Model C': 0.88, 'Model D': 0.92, 'Model E': 0.87\n",
    "    },\n",
    "    'Experiment 2': {\n",
    "        'Model A': 0.91, 'Model B': 0.89, 'Model C': 0.93, 'Model D': 0.94, 'Model E': 0.86\n",
    "    },\n",
    "    'Experiment 3': {\n",
    "        'Model A': 0.87, 'Model B': 0.9, 'Model C': 0.86, 'Model D': 0.95, 'Model E': 0.84\n",
    "    },\n",
    "    'Experiment 4': {\n",
    "        'Model A': 0.88, 'Model B': 0.85, 'Model C': 0.89, 'Model D': 0.93, 'Model E': 0.87\n",
    "    },\n",
    "    'Experiment 5': {\n",
    "        'Model A': 0.89, 'Model B': 0.88, 'Model C': 0.91, 'Model D': 0.92, 'Model E': 0.85\n",
    "    },\n",
    "    'Experiment 6': {\n",
    "        'Model A': 0.9, 'Model B': 0.87, 'Model C': 0.92, 'Model D': 0.91, 'Model E': 0.88\n",
    "    },\n",
    "    'Experiment 7': {\n",
    "        'Model A': 0.86, 'Model B': 0.89, 'Model C': 0.85, 'Model D': 0.94, 'Model E': 0.89\n",
    "    },\n",
    "    'Experiment 8': {\n",
    "        'Model A': 0.91, 'Model B': 0.92, 'Model C': 0.88, 'Model D': 0.93, 'Model E': 0.86\n",
    "    },\n",
    "    'Experiment 9': {\n",
    "        'Model A': 0.92, 'Model B': 0.87, 'Model C': 0.89, 'Model D': 0.95, 'Model E': 0.87\n",
    "    },\n",
    "    'Experiment 10': {\n",
    "        'Model A': 0.89, 'Model B': 0.9, 'Model C': 0.87, 'Model D': 0.94, 'Model E': 0.88\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1A: Count of Models Meeting Performance Thresholds\n",
    "\n",
    "**Learning Objective**: Use loops and conditionals to create a dictionary that counts the number of times models meet or exceed certain performance thresholds.\n",
    "\n",
    "**Scenario**:\n",
    "You have multiple models evaluated over several experiments, and you want to count how many times each model meets or exceeds a performance threshold (e.g., accuracy of 90%).\n",
    "\n",
    "**Instructions**:\n",
    "1. Iterate through the `model_performance` dictionary.\n",
    "2. For each model in the performance list, update their count in `performance_count_dict` if they meet or exceed the threshold."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:52:53.234112Z",
     "start_time": "2025-06-03T20:52:53.230733Z"
    }
   },
   "source": [
    "# Create a dictionary to store the count of models meeting the performance threshold\n",
    "performance_count_dict = {}\n",
    "threshold = 0.9\n",
    "\n",
    "# Iterate through the model performance dictionary\n",
    "for experiment, models in model_performance.items():\n",
    "    for model, performance in models.items():\n",
    "        if performance >= threshold:\n",
    "            if model in performance_count_dict:\n",
    "                performance_count_dict[model] += 1\n",
    "            else:\n",
    "                performance_count_dict[model] = 1\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(performance_count_dict)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model B': 4, 'Model D': 10, 'Model A': 4, 'Model C': 3}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1B: Count of Models Exceeding Different Performance Thresholds\n",
    "\n",
    "**Learning Objective**: Use loops and conditionals to create a dictionary that counts the number of times models exceed different performance thresholds.\n",
    "\n",
    "**Scenario**:\n",
    "You have multiple models evaluated over several experiments, and you want to count how many times each model exceeds various performance thresholds (e.g., 85%, 90%, 95%).\n",
    "\n",
    "**Instructions**:\n",
    "1. Iterate through the `model_performance` dictionary.\n",
    "2. For each model in the performance list, update their count in `threshold_count_dict` for each threshold they exceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:53:04.863283Z",
     "start_time": "2025-06-03T20:53:04.858810Z"
    }
   },
   "source": [
    "# Create a dictionary to store the count of models exceeding different thresholds\n",
    "thresholds = [0.85, 0.9, 0.95]\n",
    "threshold_count_dict = {threshold: {} for threshold in thresholds}\n",
    "\n",
    "# Iterate through the model performance dictionary\n",
    "for experiment, models in model_performance.items():\n",
    "    for model, performance in models.items():\n",
    "        for threshold in thresholds:\n",
    "            if performance >= threshold:\n",
    "                if model in threshold_count_dict[threshold]:\n",
    "                    threshold_count_dict[threshold][model] += 1\n",
    "                else:\n",
    "                    threshold_count_dict[threshold][model] = 1\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(threshold_count_dict)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.85: {'Model A': 10, 'Model B': 10, 'Model C': 10, 'Model D': 10, 'Model E': 9}, 0.9: {'Model B': 4, 'Model D': 10, 'Model A': 4, 'Model C': 3}, 0.95: {'Model D': 2}}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Best-Performing Models\n",
    "\n",
    "**Learning Objective**: Use loops and conditionals to determine the best-performing models across multiple experiments.\n",
    "\n",
    "**Scenario**:\n",
    "You have multiple models evaluated over several experiments, and you want to determine which models have the highest average performance.\n",
    "\n",
    "**Instructions**:\n",
    "1. Use the `model_performance` dictionary from the previous exercises.\n",
    "2. Calculate the average performance for each model.\n",
    "3. Determine the model(s) with the highest average performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T20:53:19.493862Z",
     "start_time": "2025-06-03T20:53:19.489887Z"
    }
   },
   "source": [
    "# Assuming model_performance dictionary is already created from the previous exercise\n",
    "\n",
    "# Create a dictionary to store the total performance and count of evaluations for each model\n",
    "total_performance = {}\n",
    "count_evaluations = {}\n",
    "\n",
    "# Iterate through the model performance dictionary\n",
    "for experiment, models in model_performance.items():\n",
    "    for model, performance in models.items():\n",
    "        if model in total_performance:\n",
    "            total_performance[model] += performance\n",
    "            count_evaluations[model] += 1\n",
    "        else:\n",
    "            total_performance[model] = performance\n",
    "            count_evaluations[model] = 1\n",
    "\n",
    "# Calculate the average performance for each model\n",
    "average_performance = {model: total_performance[model] / count_evaluations[model] for model in total_performance}\n",
    "\n",
    "# Find the maximum average performance\n",
    "max_average_performance = max(average_performance.values())\n",
    "\n",
    "# Create a list of models with the maximum average performance\n",
    "best_performing_models = [model for model, performance in average_performance.items() if performance == max_average_performance]\n",
    "\n",
    "# Print the resulting list\n",
    "print(best_performing_models)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model D']\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
